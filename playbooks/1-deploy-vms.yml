---
- name: Deploy Semperis Lab - 17 VMs (Async/Parallel)
  hosts: localhost
  gather_facts: no

  environment:
    ANSIBLE_ASYNC_DIR: "/tmp/semaphore/.ansible_async"

  vars:
    # Proxmox connection details
    proxmox_host1: 192.168.33.21
    proxmox_host2: 192.168.33.22
    proxmox_user: root@pam
    proxmox_password: "{{ lookup('env', 'proxmox-root-password') }}"
    validate_certs: no

    # Template IDs (clean Windows without cloudbase-init)
    # Each host has its own template (no shared storage)
    template_host1: 9001  # semperis-nuc-1
    template_host2: 9002  # semperis-nuc-2

    # Network configuration
    network_bridge: vmbr0
    network_vlan: 35

    # VM definitions
    vms:
      # Host 1 VMs (9 VMs on semperis-nuc-1)
      - { name: "ATLAS", vmid: 3510, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 4, memory: 8192, disk: "100G" }
      - { name: "SUMMIT", vmid: 3511, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 4, memory: 8192, disk: "100G" }
      - { name: "PEAK", vmid: 3512, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 4, memory: 16384, disk: "200G" }
      - { name: "RIDGE", vmid: 3513, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "500G" }
      - { name: "KOA-DC01", vmid: 3520, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "GRANITE-DC01", vmid: 3530, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "SLATE-DC01", vmid: 3532, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "MARBLE-DC01", vmid: 3534, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "MESA-R01", vmid: 3550, host: "{{ proxmox_host1 }}", node: "semperis-nuc-1", template: "{{ template_host1 }}", cores: 2, memory: 4096, disk: "80G" }

      # Host 2 VMs (8 VMs on semperis-nuc-2)
      - { name: "CREST", vmid: 3514, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "500G" }
      - { name: "SLOPE", vmid: 3519, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "OAK-DC01", vmid: 3521, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "BASALT-DC01", vmid: 3531, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "SHALE-DC01", vmid: 3533, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "QUARTZ-DC01", vmid: 3535, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "BLUFF-R01", vmid: 3551, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }
      - { name: "CLIFF-R01", vmid: 3552, host: "{{ proxmox_host2 }}", node: "semperis-nuc-2", template: "{{ template_host2 }}", cores: 2, memory: 4096, disk: "80G" }

  tasks:
    - name: Display deployment plan
      debug:
        msg:
          - "=========================================="
          - "Deploying 17 VMs to Semperis Lab (ASYNC)"
          - "=========================================="
          - "Host 1 ({{ proxmox_host1 }}): 9 VMs"
          - "Host 2 ({{ proxmox_host2 }}): 8 VMs"
          - "All VMs will be cloned in parallel!"
          - "=========================================="

    # ============================================
    # PHASE 1: Clone VMs (ALL IN PARALLEL)
    # ============================================
    - name: Clone VMs from templates (async/parallel)
      community.general.proxmox_kvm:
        api_host: "{{ item.host }}"
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        validate_certs: "{{ validate_certs }}"
        node: "{{ item.node }}"
        vmid: "{{ item.template }}"
        clone: "{{ item.template }}"
        newid: "{{ item.vmid }}"
        name: "{{ item.name }}"
        full: yes
        timeout: 600
      loop: "{{ vms }}"
      async: 600  # Max 10 minutes per clone
      poll: 0     # Don't wait, launch all in parallel
      register: clone_jobs

    - name: Display clone jobs launched
      debug:
        msg: "Launched {{ clone_jobs.results | length }} clone jobs in parallel. Waiting for completion..."

    - name: Brief pause to let async jobs initialize
      pause:
        seconds: 2
        prompt: "Giving async jobs a moment to start..."

    - name: Wait for all clone jobs to complete
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: clone_results
      until: clone_results.finished
      retries: 120  # 120 retries * 5 seconds = 10 minutes max
      delay: 5
      loop: "{{ clone_jobs.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Display clone completion
      debug:
        msg: "All VMs cloned successfully!"

    # ============================================
    # PHASE 2: Configure VLAN tags (parallel)
    # ============================================
    - name: Configure VLAN tags on network interfaces (async/parallel)
      community.general.proxmox_nic:
        api_host: "{{ item.host }}"
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        validate_certs: "{{ validate_certs }}"
        vmid: "{{ item.vmid }}"
        interface: net0
        bridge: "{{ network_bridge }}"
        tag: "{{ network_vlan }}"
      loop: "{{ vms }}"
      async: 60
      poll: 0
      register: vlan_jobs

    - name: Wait for all VLAN configuration jobs to complete
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: vlan_results
      until: vlan_results.finished
      retries: 30
      delay: 2
      loop: "{{ vlan_jobs.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Display VLAN configuration completion
      debug:
        msg: "All VLAN tags configured!"

    # ============================================
    # PHASE 3: Configure VM hardware (parallel)
    # ============================================
    - name: Configure VM hardware (async/parallel)
      community.general.proxmox_kvm:
        api_host: "{{ item.host }}"
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        validate_certs: "{{ validate_certs }}"
        vmid: "{{ item.vmid }}"
        node: "{{ item.node }}"
        name: "{{ item.name }}"
        cores: "{{ item.cores }}"
        memory: "{{ item.memory }}"
        update: yes
      loop: "{{ vms }}"
      async: 60
      poll: 0
      register: hardware_jobs

    - name: Wait for all hardware configuration jobs to complete
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: hardware_results
      until: hardware_results.finished
      retries: 30
      delay: 2
      loop: "{{ hardware_jobs.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Display hardware configuration completion
      debug:
        msg: "All VM hardware configured!"

    # ============================================
    # PHASE 4: Resize disks (parallel)
    # ============================================
    - name: Resize VM disks (async/parallel)
      community.general.proxmox_disk:
        api_host: "{{ item.host }}"
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        validate_certs: "{{ validate_certs }}"
        vmid: "{{ item.vmid }}"
        disk: scsi0
        size: "{{ item.disk }}"
        state: resized
      loop: "{{ vms }}"
      async: 120
      poll: 0
      register: disk_jobs

    - name: Wait for all disk resize jobs to complete
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: disk_results
      until: disk_results.finished
      retries: 60
      delay: 2
      loop: "{{ disk_jobs.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Display disk resize completion
      debug:
        msg: "All VM disks resized!"

    # ============================================
    # PHASE 5: Start VMs (parallel)
    # ============================================
    - name: Start all VMs (async/parallel)
      community.general.proxmox_kvm:
        api_host: "{{ item.host }}"
        api_user: "{{ proxmox_user }}"
        api_password: "{{ proxmox_password }}"
        validate_certs: "{{ validate_certs }}"
        vmid: "{{ item.vmid }}"
        node: "{{ item.node }}"
        state: started
      loop: "{{ vms }}"
      async: 60
      poll: 0
      register: start_jobs

    - name: Wait for all VM start jobs to complete
      async_status:
        jid: "{{ item.ansible_job_id }}"
      register: start_results
      until: start_results.finished
      retries: 30
      delay: 2
      loop: "{{ start_jobs.results }}"
      loop_control:
        label: "{{ item.item.name }}"

    - name: Display VM start completion
      debug:
        msg: "All VMs started!"

    # ============================================
    # Wait for VMs to boot
    # ============================================
    - name: Wait for VMs to boot
      pause:
        seconds: 60
        prompt: "Waiting for all VMs to boot and get DHCP addresses..."

    - name: Display completion summary
      debug:
        msg:
          - "=========================================="
          - "VM Deployment Complete! (Async Mode)"
          - "=========================================="
          - ""
          - "All 17 VMs have been deployed and started."
          - "VMs are booting with DHCP addresses."
          - ""
          - "Performance improvement:"
          - "  - VMs cloned in parallel (much faster!)"
          - "  - All configuration steps parallelized"
          - ""
          - "Next step:"
          - "  Run playbook: 2-configure-ips.yml"
          - ""
          - "This will:"
          - "  1. Find DHCP IPs of all VMs"
          - "  2. Set static IPs on VLAN 35"
          - "  3. Configure hostnames"
          - "  4. Set DNS servers"
          - "=========================================="